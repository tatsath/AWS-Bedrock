{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation and Meta-Evaluation with GroUSE\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces GroUSE, a framework for evaluating Retrieval-Augmented Generation (RAG) pipelines, focusing on the final stage: Grounded Question Answering (GQA). It demonstrates how to use Large Language Models (LLMs) to assess GQA answers across four distinct metrics and guides you through customizing your own Judge LLM using GroUSE unit tests.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Manually evaluating RAG pipeline outputs can be challenging. The GroUSE framework leverages LLMs with finely tuned prompts to address all potential failure modes in Grounded Question Answering. GroUSE unit tests are used to identify the most effective prompts to optimize the performance of these evaluators.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Answer Relevancy evaluation\n",
    "2. Completeness evaluation\n",
    "3. Faithfulness evaluation\n",
    "4. Usefulness evaluation\n",
    "5. Judge LLM Customization\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### The task we want to assess: Grounded Question Answering\n",
    "\n",
    "Grounded Question Answering (QA) is usually the last step of a RAG pipeline: given a question and a set of documents retrieved from the corpus, an LLM must generate an answer. We expect the LLM to cite which document each piece of information is coming from, as depicted below. When no precise answer is in the documents, the LLM should indicate it in its answer. In that case, if some related information is available in the documents, the LLM can add it to the answer to show the corpus is not completely off-topic with respect to the question.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Each answer is evaluated according to six metrics. The fisrt four metrics are evaluated with an evaluator LLM call. Positive acceptance and negative rejection are deducted from the first four. \n",
    "\n",
    "#### 1. Answer Relevancy\n",
    "\n",
    "Answer relevancy assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5).\n",
    "\n",
    "#### 2. Completeness\n",
    "\n",
    "Completeness uses a Likert scale (1 to 5) to evaluate whether all relevant information from the documents is present in the answer.\n",
    "\n",
    "#### 3. Faithfulness\n",
    "\n",
    "Faithfulness is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document.\n",
    "\n",
    "#### 4. Usefulness\n",
    "\n",
    "When the answer states that no references can answer the question but additional information is provided, usefulness is a binary score that determines if the provided additional information is still useful.\n",
    "\n",
    "#### 5. Positive Acceptance\n",
    "\n",
    "Percentage of samples that responded when they were supposed to.\n",
    "\n",
    "#### 6. Negative Rejection\n",
    "\n",
    "Percentage of samples that refrained from responding when there is no context in the documents that allow to answer the question.\n",
    "\n",
    "## Benefits of the approach\n",
    "\n",
    "The GroUSE framework comprehensively addresses the seven failure modes of Grounded Question Answering, providing a thorough evaluation of your RAG pipeline's final stage.\n",
    "\n",
    "## Implementation details\n",
    "\n",
    "Answer Relevancy, Completeness, Faithfulness and Usefulness are evaluated using GPT-4 as the default model, as it was the best model we tested.\n",
    "Positive acceptance and negative rejection can be deducted from the answer relevancy and completeness results as these can have `None` values when no references contain answers to the question.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The GroUSE framework provides a comprehensive set of evaluation metrics to assess the performance of Grounded Question Answering models. By addressing seven key failure modes, it enables developers to thoroughly evaluate and improve their RAG pipelines. The use of LLM-based judges, such as GPT-4, automate this evaluation process. To tailor the framework to your specific needs, you can develop a custom LLM evaluator and validate its performance using GroUSE unit tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"../images/grouse.svg\" alt=\"grouse\" style=\"width:100%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TODO add Mermaid schema -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from grouse import (\n",
    "    EvaluationSample,\n",
    "    GroundedQAEvaluator,\n",
    "    meta_evaluate_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid nested asyncio loops inside notebooks (this line is not needed if you run the code in a Python script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, you will need access to the OpenAI API and get an OpenAI API key. You can get one [here](https://platform.openai.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the evaluator\n",
    "\n",
    "The default model used is [GPT-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4). Prompts are adapted to this model, so if you want to have the best results, keep using the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GroundedQAEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a good answer\n",
    "\n",
    "An LLM has given a good answer to a question related to the Eiffel Tower, given some contexts from the [Eiffel Tower Wikipedia](https://en.wikipedia.org/wiki/Eiffel_Tower) page. Let's evaluate the answer and check that everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 163.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 10:04:36,799 - LLM Call Tracker - INFO - Cost: 1.3518$\n",
      "2025-04-26 10:04:36,799 - LLM Call Tracker - INFO - Cost: 1.3518$\n",
      "2025-04-26 10:04:36,799 - LLM Call Tracker - INFO - Cost: 1.3518$\n",
      "Answer Relevancy (1 to 5): 5\n",
      "Answer Relevancy (1 to 5): The answer directly responds to the user's question about the location of the Eiffel Tower.\n",
      "Completeness (1 to 5): 5\n",
      "Completeness (1 to 5): The answer includes all the relevant information from the reference, which is the location of the Eiffel Tower.\n",
      "Faithfulness (0 or 1): 1\n",
      "Faithfulness (0 or 1): The sentence in the answer correctly cites its source, the information provided is in agreement with the cited source, and it does not distort or modify the content of the reference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/tatsa/miniforge3/envs/ecc/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/tatsa/miniforge3/envs/ecc/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "good_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower stands in the Champs de Mars in Paris.[1]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris. [1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[good_sample]).evaluations[0]\n",
    "\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy)\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy_justification)\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness)\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness_justification)\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness)\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness_justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_results(result):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Extract scores and justifications from result\n",
    "    scores = {\n",
    "        'Answer Relevancy': result.answer_relevancy.answer_relevancy,\n",
    "        'Completeness': result.completeness.completeness,\n",
    "        'Faithfulness': result.faithfulness.faithfulness\n",
    "    }\n",
    "    justifications = {\n",
    "        'Answer Relevancy': result.answer_relevancy.answer_relevancy_justification,\n",
    "        'Completeness': result.completeness.completeness_justification,\n",
    "        'Faithfulness': result.faithfulness.faithfulness_justification\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Metric': list(scores.keys()),\n",
    "        'Score': list(scores.values()),\n",
    "        'Justification': list(justifications.values())\n",
    "    })\n",
    "\n",
    "\n",
    "    # Show full justification text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(\"\\nEvaluation Results Table:\")\n",
    "    styled_df = results_df.style.set_properties(**{'text-align': 'left'})\n",
    "    display(styled_df)\n",
    "    \n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(scores.keys(), scores.values(), color='skyblue', width=0.6)\n",
    "\n",
    "    # Add value labels above bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{height:.2f}',\n",
    "                 ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Horizontal grid lines at each integer score\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.yticks(range(0, 6), fontsize=12)\n",
    "    plt.xticks(rotation=0, fontsize=12)  # Horizontal metric names\n",
    "\n",
    "    plt.ylim(0, 5.5)\n",
    "    plt.title('Evaluation Metrics Scores', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Score (1-5 scale)', fontsize=12)\n",
    "\n",
    "    # Add score descriptions at the bottom, horizontally\n",
    "    plt.figtext(0.5, -0.08,\n",
    "                \"Score Descriptions:  \"\n",
    "                \"Answer Relevancy = How well the answer matches the question;  \"\n",
    "                \"Completeness = How comprehensive the answer is;  \"\n",
    "                \"Faithfulness = How accurately the answer reflects the source material.\",\n",
    "                ha='center', fontsize=11, style='italic', wrap=True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space for bottom text\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8fdff_row0_col0, #T_8fdff_row0_col1, #T_8fdff_row0_col2, #T_8fdff_row1_col0, #T_8fdff_row1_col1, #T_8fdff_row1_col2, #T_8fdff_row2_col0, #T_8fdff_row2_col1, #T_8fdff_row2_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8fdff\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8fdff_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_8fdff_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "      <th id=\"T_8fdff_level0_col2\" class=\"col_heading level0 col2\" >Justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8fdff_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8fdff_row0_col0\" class=\"data row0 col0\" >Answer Relevancy</td>\n",
       "      <td id=\"T_8fdff_row0_col1\" class=\"data row0 col1\" >5</td>\n",
       "      <td id=\"T_8fdff_row0_col2\" class=\"data row0 col2\" >The answer directly responds to the user's question about the location of the Eiffel Tower.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8fdff_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8fdff_row1_col0\" class=\"data row1 col0\" >Completeness</td>\n",
       "      <td id=\"T_8fdff_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "      <td id=\"T_8fdff_row1_col2\" class=\"data row1 col2\" >The answer includes all the relevant information from the reference, which is the location of the Eiffel Tower.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8fdff_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8fdff_row2_col0\" class=\"data row2 col0\" >Faithfulness</td>\n",
       "      <td id=\"T_8fdff_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "      <td id=\"T_8fdff_row2_col2\" class=\"data row2 col2\" >The sentence in the answer correctly cites its source, the information provided is in agreement with the cited source, and it does not distort or modify the content of the reference.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3529ad270>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_evaluation_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it behave with an irrelevant answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 201.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 10:01:37,885 - LLM Call Tracker - INFO - Cost: 1.2537$\n",
      "2025-04-26 10:01:37,885 - LLM Call Tracker - INFO - Cost: 1.2537$\n",
      "2025-04-26 10:01:37,885 - LLM Call Tracker - INFO - Cost: 1.2537$\n",
      "Answer Relevancy (1 to 5): 1\n",
      "Justification: The answer provides information about the material of the Eiffel Tower, which is not relevant to the user's question about its location.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/tatsa/miniforge3/envs/ecc/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/tatsa/miniforge3/envs/ecc/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "irrelevant_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is mainly made of puddle iron.[2]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris.[1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"The puddle iron (wrought iron) of the Eiffel Tower weighs 7,300 tonnes,[70] and the addition of lifts, shops and antennae have brought the total weight to approximately 10,100 tonnes.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[irrelevant_sample]).evaluations[0]\n",
    "\n",
    "print(\"Answer Relevancy (1 to 5):\", result.answer_relevancy.answer_relevancy)\n",
    "print(\"Justification:\", result.answer_relevancy.answer_relevancy_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of an incomplete sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:28:56,243 - LLM Call Tracker - INFO - Cost: 0.3269$\n",
      "Completeness (1 to 5): 2\n",
      "Justification: This answer only includes part of the relevant information from the references. It mentions the criticism from those who did not believe the project was feasible and some artists. However, it does not mention the formation of the artist committee, the leader of the committee, or the names of the important figures of the arts who were part of the committee.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "incomplete_sample = EvaluationSample(\n",
    "    input=\"Who critized the Eiffel Tower project in 1889?\",\n",
    "    actual_output=(\n",
    "        \"The tower was critized by those who did not believe it was feasible and some artists.[1]\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"The tower was critized by those who did not believe it was feasible and those who objected on artistic grounds.[1]\"\n",
    "        \"An artist committee was created to protest againt the construction of the tower, led by the prominent architect \"\n",
    "        \"Charles Garnier and including some of the most important figures of the arts, \"\n",
    "        \"such as William-Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet. [2]\"\n",
    "    ),\n",
    "    references=[\n",
    "        \"The proposed tower had been a subject of controversy, drawing criticism from those who did not believe it was feasible and those who objected on artistic grounds.\",\n",
    "        (\n",
    "            \"It came to a head as work began at the Champ de Mars: a \\\"Committee of Three Hundred\\\" \"\n",
    "            \"(one member for each metre of the tower's height) was formed, led by the prominent architect \"\n",
    "            \"Charles Garnier and including some of the most important figures of the arts, \"\n",
    "            \"such as William-Adolphe Bouguereau, Guy de Maupassant, Charles Gounod and Jules Massenet.\"\n",
    "        ),\n",
    "        \"A petition called \\\"Artists against the Eiffel Tower\\\" was sent to the Minister of Works and Commissioner for the Exposition, Adolphe Alphand, and it was published by Le Temps on 14 February 1887\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[incomplete_sample]).evaluations[0]\n",
    "\n",
    "print(\"Completeness (1 to 5):\", result.completeness.completeness)\n",
    "print(\"Justification:\", result.completeness.completeness_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of an unfaithful sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:29:14,867 - LLM Call Tracker - INFO - Cost: 0.4289$\n",
      "Faithfulness (0 or 1): 0\n",
      "Justification: The sentence in the answer cites its sources, but the information provided is not in agreement with the cited sources. The Eiffel Tower is not located at Rue Rabelais in Paris.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unfaithful_sample = EvaluationSample(\n",
    "    input=\"Where is the Eiffel Tower located?\",\n",
    "    actual_output=\"The Eiffel Tower is located at Rue Rabelais in Paris.[1][2]\",\n",
    "    expected_output=\"In the Champs de Mars in Paris.[1]\",\n",
    "    references=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France\",\n",
    "        \"Gustave Eiffel died in his appartment at Rue Rabelais in Paris.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[unfaithful_sample]).evaluations[0]\n",
    "\n",
    "print(\"Faithfulness (0 or 1):\", result.faithfulness.faithfulness)\n",
    "print(\"Justification:\", result.faithfulness.faithfulness_justification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of information utility in case there is no answer to the question in the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:29:46,392 - LLM Call Tracker - INFO - Cost: 0.5763$\n",
      "Usefulness (0 or 1): 1\n",
      "Justification: The related information provided is directly linked to the user's question. It does not name the critics of the Eiffel Tower project in 1889, but it does mention a petition against the project, which implies criticism.\n",
      "Positive Acceptance: 1\n",
      "Negative Rejection: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "useful_sample = EvaluationSample(\n",
    "    input=\"Who critized the Eiffel Tower project in 1889?\",\n",
    "    actual_output=(\n",
    "        \"No document seems to precisely answer your question.\"\n",
    "        \"However, it is mentioned that a petition against tht Eiffel Tower construciton was sent \"\n",
    "        \"to the Minister of Works and Commissioner for the Exposition [1]\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"No document seems to precisely answer your question.\"\n",
    "        \"However, it is worth noting that a petition against tht Eiffel Tower construciton was sent \"\n",
    "        \"to the Minister of Works and Commissioner for the Exposition [1]\"\n",
    "    ),\n",
    "    references=[\n",
    "        \"A petition against the tower was sent to the Minister of Works and Commissioner for the Exposition, Adolphe Alphand, and it was published by Le Temps on 14 February 1887\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(eval_samples=[useful_sample]).evaluations[0]\n",
    "\n",
    "print(\"Usefulness (0 or 1):\", result.usefulness.usefulness)\n",
    "print(\"Justification:\", result.usefulness.usefulness_justification)\n",
    "print(\"Positive Acceptance:\", result.positive_acceptance)\n",
    "print(\"Negative Rejection:\", result.negative_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that all the results are cached and we can compute the global statistics on all the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 133.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:29:46,435 - LLM Call Tracker - INFO - Cost: 1.1526$\n",
      "Average answer relevancy:  3.0\n",
      "Average completeness:  2.25\n",
      "Average faithfulness:  0.8\n",
      "Average usefulness:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_report = evaluator.evaluate(eval_samples=[\n",
    "    good_sample,\n",
    "    irrelevant_sample,\n",
    "    incomplete_sample,\n",
    "    unfaithful_sample,\n",
    "    useful_sample,\n",
    "]).report\n",
    "print(\"Average answer relevancy: \", evaluation_report.answer_relevancy)\n",
    "print(\"Average completeness: \", evaluation_report.completeness)\n",
    "print(\"Average faithfulness: \", evaluation_report.faithfulness)\n",
    "print(\"Average usefulness: \", evaluation_report.usefulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own Judge LLM\n",
    "\n",
    "Since GPT-4 is expensive, let's create a new evaluator using gpt-4o-mini. For that, we need to adapt the evaluation prompt to the model by using the train set of the [GroUSE unit tests](https://huggingface.co/datasets/illuin/grouse).\n",
    "Make sure that the output follows the same format as described in the prompts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evaluation_prompt = \"\"\"# Task\n",
    "\n",
    "Task: Grounded Question Answering\n",
    "Based solely on the content of the references, the objective is to generate a response to the user's query. Each statement must be followed by the reference of the source passage, in the format [i] where i is the number of the reference. If no passage seems relevant, the answer should begin with \"No document seems to precisely answer your question\" and may be supplemented with related sourced information.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "I will provide you with two answers, numbered 1 and 2, each containing a response to the user request.\n",
    "I want you to assign to each answer a relevancy grade between 1 and 5:\n",
    "- Answer relevancy evaluates if the content of the answer accurately responds to the user's question.\n",
    "- The truthfulness of the information in the answer does not impact relevancy: even if information that appears false is contained in the answer, as long as this information is related to the request, then relevancy should not decrease. Remember that this information could come from references mentioning imaginary content that you are unaware of: the only thing to evaluate to assign the relevancy grade is therefore the adequacy between the information in the answer and the request, NOT their truthfulness.\n",
    "- The absence of information in the answer does not impact relevancy, only the information contained in the answer is evaluated.\n",
    "- Answer relevancy cannot be evaluated if the answer mentions that no document responds to the user request, it is then `null`, regardless of whether it contains other information or not.\n",
    "\n",
    "Rating scale:\n",
    "null - The answer asserts that no document precisely responds to the user request. Even if it provides additional information, whether appropriate or not, the relevancy remains `null`.\n",
    "5 - The answer has excellent relevancy. All information provided in the answer is in line with the question and precisely answers the user request.\n",
    "4 - The answer achieves good relevancy by providing relevant information to answer the user question. Some information indicated does not exactly answer the question, but remains in line with the request.\n",
    "3 - The answer has average relevancy, it contains information that allows responding to the user request, but it also contains superfluous information, which was not necessary to answer the request.\n",
    "2 - The answer shows low relevancy, with some elements related to the request, but the majority of the content is not in line with the question asked.\n",
    "1 - The answer has very low relevancy, not answering the user's question at all. The content is largely inappropriate or off-topic, delivering no useful information for the request.\n",
    "\n",
    "Before assigning each grade, you will check that the answer does not contain \"No document responds...\", if this is the case you must put a grade of `null`. If this is not the case, you will then analyze the adequacy between the request and the information contained in the answer.\n",
    "Your response should be in JSON format, respecting the following format:\n",
    "{\n",
    "    \"answer_1\": {\n",
    "        \"answer_affirms_no_document_answers\": X,\n",
    "        \"answer_relevancy_justification\": \"...\",\n",
    "        \"answer_relevancy\": Y\n",
    "    },\n",
    "    \"answer_2\": {\n",
    "        \"answer_affirms_no_document_answers\": X,\n",
    "        \"answer_relevancy_justification\": \"...\",\n",
    "        \"answer_relevancy\": Y\n",
    "    }\n",
    "}\n",
    "Where \"...\" is a string, X is a boolean, and Y is an integer between 1 and 5 or `null`.\n",
    "\n",
    "# Sample\n",
    "\n",
    "User request: {{ input }}\n",
    "\n",
    "# To evaluate\n",
    "\n",
    "Answer 1: {{ expected_output }}\n",
    "Answer 2: {{ actual_output }}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_evaluation_prompt = \"\"\"# Task\n",
    "\n",
    "Task: Grounded Question Answering\n",
    "Based solely on the content of the references, the objective is to generate a response to the user's query. Each statement must be followed by the reference of the source passage, in the format [i] where i is the number of the reference. If no passage seems relevant, the answer should begin with \"No document seems to precisely answer your question\" and may be supplemented with related sourced information.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "I will provide you with two answers, numbered 1 and 2, each containing a response to the user request.\n",
    "I want you to assign to each answer a completeness grade between 1 and 5:\n",
    "- The only condition for an answer to be complete is the presence in it of at least all the information from the references that are relevant to the question asked.\n",
    "- The presence of unrelated information in the answer does not impact completeness.\n",
    "- The presence of information in the answer not from the references does not impact completeness.\n",
    "- Possible errors in the sources citing the references do not impact completeness.\n",
    "- Completeness cannot be evaluated if the references contain no information that can precisely answer the user request, in which case the grade takes the value `null`.\n",
    "\n",
    "Rating scale:\n",
    "null - The references contained no relevant information to precisely answer the user's question. In this case, there is no need to read the content of the answer to know that the grade is `null`.\n",
    "5 - The answer is very complete, it contains all the relevant information from the references. No essential information is omitted, ensuring complete coverage of the question asked.\n",
    "4 - The answer covers most of the relevant information in depth. It integrates the references satisfactorily, covering the majority of key points. Some details may be missing, but overall, the answer is substantial.\n",
    "3 - The answer reasonably addresses a number of relevant aspects. It integrates part of the necessary information from the references. However, gaps remain, impacting the overall completeness.\n",
    "2 - The answer only covers a minimal part of the relevant information. It misses several important information from the references.\n",
    "1 - The answer covers none of the relevant information, all relevant information from the references has been omitted in the answer.\n",
    "\n",
    "Before assigning each grade, you will always start by analyzing the information found in the references that are relevant to the user request. If there is no relevant information in the references, completeness must be `null`. If there are relevant information in the references, you will analyze which portion of this information is present or absent in the answers to evaluate the completeness grade. Your response should be in JSON format, respecting the following format:\n",
    "{\n",
    "    \"answer_1\": {\n",
    "        \"completeness_justification\": \"...\",\n",
    "        \"completeness\": X\n",
    "    },\n",
    "    \"answer_2\": {\n",
    "        \"completeness_justification\": \"...\",\n",
    "        \"completeness\": X\n",
    "    }\n",
    "}\n",
    "Where \"...\" is a string, and X is an integer between 1 and 5 or `null`.\n",
    "\n",
    "# SAMPLE\n",
    "\n",
    "List of references :\n",
    "{%- for context in contexts %}\n",
    "Reference {{ loop.index }}: {{ context }}\n",
    "{%- endfor %}\n",
    "User request: {{ input }}\n",
    "\n",
    "# To evaluate\n",
    "\n",
    "Answer 1: {{ expected_output }}\n",
    "Answer 2: {{ actual_output }}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evaluation_prompt = \"\"\"# Task\n",
    "\n",
    "Task: Grounded Question Answering\n",
    "Based solely on the content of the references, the objective is to generate a response to the user's query. Each statement must be followed by the reference of the source passage, in the format [i] where i is the number of the reference. If no passage seems relevant, the answer should begin with \"No document seems to precisely answer your question\" and may be supplemented with related sourced information.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "I will provide you with two answers, numbered 1 and 2, each containing a response to the user request.\n",
    "I want you to assign to each answer a boolean faithfulness grade. An answer is faithful if:\n",
    "- Each statement made by the answer is followed by a source indicating the reference from which it is drawn.\n",
    "- The information preceding the source is indeed from the corresponding reference.\n",
    "- The information preceding the source is in agreement with the corresponding reference, and does not assert facts different from those indicated in the reference.\n",
    "In all other cases, the response is considered non-faithful.\n",
    "Faithfulness is also considered non-measurable if the answer asserts that no document responds to the question, and it does not provide any related information, it is then `null`.\n",
    "\n",
    "Rating scale:\n",
    "null - The answer asserts that no document responds to the question, and does not provide any related information.\n",
    "1 - All sentences in the answer cite their sources, and are in agreement with the cited sources.\n",
    "0 - At least one sentence in the response does not cite its sources, or cites a wrong source, or modifies the content from the references, or asserts something that is not supported by the cited references.\n",
    "\n",
    "Before assigning each grade, you will start by verifying that the answer does not only assert \"No document responds...\", without any other information. If this is the case, then faithfulness must be `null`. Otherwise, I want you to analyze by explaining for each sentence, one after the other, if 1) a reference follows the sentence, 2) the reference following the sentence is correct, and 3) if the sentence does not distort or modify the content of the references. Your response should be in JSON format, respecting the following format:\n",
    "{\n",
    "    \"answer_1\": {\n",
    "        \"answer_only_asserts_no_document_answers\": X,\n",
    "        \"content_analysis_sentence_by_sentence\": [\n",
    "            {\n",
    "                \"sentence\": \"...\",\n",
    "                \"criterion_1\": \"...\",\n",
    "                \"criterion_2\": \"...\",\n",
    "                \"criterion_3\": \"...\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"faithfulness_justification\": \"...\",\n",
    "        \"faithfulness\": Y\n",
    "    },\n",
    "    \"answer_2\": {\n",
    "        \"answer_only_asserts_no_document_answers\": X,\n",
    "        \"content_analysis_sentence_by_sentence\": [\n",
    "            {\n",
    "            \"sentence\": \"...\",\n",
    "            \"criterion_1\": \"...\",\n",
    "            \"criterion_2\": \"...\",\n",
    "            \"criterion_3\": \"...\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"faithfulness_justification\": \"...\",\n",
    "        \"faithfulness\": Y\n",
    "    }\n",
    "}\n",
    "Where \"...\" is a string, X is a boolean, and Y is either a boolean or `null`.\n",
    "\n",
    "# Sample\n",
    "\n",
    "List of references :\n",
    "{%- for context in contexts %}\n",
    "Reference {{ loop.index }}: {{ context }}\n",
    "{%- endfor %}\n",
    "\n",
    "# To evaluate\n",
    "\n",
    "Answer 1: {{ expected_output }}\n",
    "Answer 2: {{ actual_output }}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefulness Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulness_evaluation_prompt = \"\"\"# Task\n",
    "\n",
    "Task: Grounded Question Answering\n",
    "Based solely on the content of the references, the objective is to generate a response to the user's query. Each statement must be followed by the reference of the source passage, in the format [i] where i is the number of the reference. If no passage seems relevant, the answer should begin with \"No document seems to precisely answer your question\" and may be supplemented with related sourced information.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "I will provide you with two answers, numbered 1 and 2, each containing a response to the user request.\n",
    "I want you to assign to each answer a usefulness grade of 0 or 1:\n",
    "- Usefulness is only evaluated when the answer says that no document precisely answers the user's question, but it still provides information related to the question.\n",
    "- Usefulness measures how interesting the related information is to know for the user, given that there is no answer in the references.\n",
    "- If the answer responds to the user request, usefulness must be `null`.\n",
    "- If the answer indicates that no document responds to the user request, without adding other information, usefulness must be `null`.\n",
    "\n",
    "Rating scale:\n",
    "null - (The answer responds to the user request) OR (the answer does not answer the user's question AND does not provide any related information).\n",
    "1 - The related information is generally related to the question and adds value to the general understanding of the topic.\n",
    "0 - The related information is completely off-topic with respect to the question asked.\n",
    "\n",
    "Before assigning each grade, you will start by verifying that the answer indeed asserts \"No document responds...\", then you will check that the answer contains related information in addition to this assertion. If one of these two conditions is `false` then usefulness must be `null`.\n",
    "If both conditions are indeed true, then you will analyze the usefulness of having added this related information to evaluate the usefulness grade. Your response should be in JSON format, respecting the following format:\n",
    "{\n",
    "    \"answer_1\": {\n",
    "        \"answer_affirms_no_document_answers\": X,\n",
    "        \"answer_contains_related_information\": X,\n",
    "        \"usefulness_justification\": \"...\",\n",
    "        \"usefulness\": Y\n",
    "    },\n",
    "    \"answer_2\": {\n",
    "        \"answer_affirms_no_document_answers\": X,\n",
    "        \"answer_contains_related_information\": X,\n",
    "        \"usefulness_justification\": \"...\",\n",
    "        \"usefulness\": Y\n",
    "    }\n",
    "}\n",
    "Where \"...\" is a string, X is a boolean, and Y is an integer that is 0 or 1 or `null`.\n",
    "\n",
    "# Sample\n",
    "\n",
    "User request: {{ input }}\n",
    "\n",
    "# To evaluate\n",
    "\n",
    "Answer 1: {{ expected_output }}\n",
    "Answer 2: {{ actual_output }}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prompts to use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_path = \"gpt4o_mini_prompts\"\n",
    "os.makedirs(prompts_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(prompts_path, \"answer_relevancy.txt.jinja\"), \"w\") as file:\n",
    "    file.write(relevancy_evaluation_prompt)\n",
    "with open(os.path.join(prompts_path, \"completeness.txt.jinja\"), \"w\") as file:\n",
    "    file.write(completeness_evaluation_prompt)\n",
    "with open(os.path.join(prompts_path, \"faithfulness.txt.jinja\"), \"w\") as file:\n",
    "    file.write(faithfulness_evaluation_prompt)\n",
    "with open(os.path.join(prompts_path, \"usefulness.txt.jinja\"), \"w\") as file:\n",
    "    file.write(usefulness_evaluation_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067c95e90e4e43f5be6e7bfd914b5935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671b3b478a4c4700adb0c63f92d5420a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/77.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a775ca8a28457bb846fed23fe72cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/749k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303199d11138433885560ce0f4aadfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ff04beed604c3ea7130caec8936609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:26<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:30:19,985 - LLM Call Tracker - INFO - Cost: 0.0148$\n",
      "2025-04-26 09:30:19,985 - LLM Call Tracker - INFO - Cost: 0.0148$\n",
      "Aggregated metrics\n",
      "answer_relevancy_success=1.0 completeness_success=0.75 faithfulness_success=0.625 usefulness_success=0.9375 positive_acceptance_success=0.8125 negative_rejection_success=0.75 total=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta_evaluations = meta_evaluate_pipeline(\"gpt-4o-mini\", prompts_path, train_set=True)\n",
    "print(\"Aggregated metrics\")\n",
    "print(meta_evaluations.report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an encouraging beginning. But, we can iterate on the prompts above to try to have better scores with GPT-4o-mini. Still, it will be difficult to have a performance as good as GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you are happy with your prompts, you can evaluate the Judge model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 93/144 [01:43<01:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-JV6yPaNcUy5jq93eBhjTM7ed on tokens per min (TPM): Limit 200000, Used 199578, Requested 2218. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/llms/openai/openai.py:790\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream)\u001b[0m\n\u001b[1;32m    777\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    779\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_aclient\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m     },\n\u001b[1;32m    788\u001b[0m )\n\u001b[0;32m--> 790\u001b[0m headers, response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_openai_chat_completion_request(\n\u001b[1;32m    791\u001b[0m     openai_aclient\u001b[38;5;241m=\u001b[39mopenai_aclient,\n\u001b[1;32m    792\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    794\u001b[0m     logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[1;32m    795\u001b[0m )\n\u001b[1;32m    796\u001b[0m stringified_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py:135\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/llms/openai/openai.py:436\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/llms/openai/openai.py:418\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[0;34m(self, openai_aclient, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    419\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    420\u001b[0m         )\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    422\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_legacy_response.py:381\u001b[0m, in \u001b[0;36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2032\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2031\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2034\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2035\u001b[0m         {\n\u001b[1;32m   2036\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2039\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2040\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2041\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2042\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2043\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2044\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2045\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2046\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2048\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2049\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2050\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2051\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2052\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2053\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2054\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2055\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2056\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2059\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2060\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2061\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2062\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2063\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2064\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2065\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2066\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2067\u001b[0m         },\n\u001b[1;32m   2068\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2069\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2070\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2071\u001b[0m     ),\n\u001b[1;32m   2072\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2073\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2074\u001b[0m     ),\n\u001b[1;32m   2075\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2076\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2077\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2078\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1805\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1802\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1803\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1804\u001b[0m )\n\u001b[0;32m-> 1805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1495\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1496\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1497\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1498\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1499\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1500\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1501\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1585\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m         input_options,\n\u001b[1;32m   1587\u001b[0m         cast_to,\n\u001b[1;32m   1588\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1589\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1590\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1591\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1632\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1633\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1634\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1635\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1636\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1637\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1585\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1586\u001b[0m         input_options,\n\u001b[1;32m   1587\u001b[0m         cast_to,\n\u001b[1;32m   1588\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1589\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1590\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1591\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1592\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1632\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1633\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1634\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1635\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1636\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1637\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/openai/_base_client.py:1600\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1603\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1604\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1609\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-JV6yPaNcUy5jq93eBhjTM7ed on tokens per min (TPM): Limit 200000, Used 199578, Requested 2218. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/main.py:477\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutine(init_response):\n\u001b[0;32m--> 477\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/llms/openai/openai.py:837\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.acompletion\u001b[0;34m(self, data, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream)\u001b[0m\n\u001b[1;32m    835\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    838\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    839\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m    840\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m    841\u001b[0m     body\u001b[38;5;241m=\u001b[39mexception_body,\n\u001b[1;32m    842\u001b[0m )\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-JV6yPaNcUy5jq93eBhjTM7ed on tokens per min (TPM): Limit 200000, Used 199578, Requested 2218. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m meta_evaluations \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_evaluate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m meta_evaluations\u001b[38;5;241m.\u001b[39mreport\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/meta_evaluator.py:167\u001b[0m, in \u001b[0;36mmeta_evaluate_pipeline\u001b[0;34m(model_name, prompts_path, train_set)\u001b[0m\n\u001b[1;32m    164\u001b[0m evaluation_samples, conditions \u001b[38;5;241m=\u001b[39m load_unit_tests(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_set \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m GroundedQAEvaluator(model_name, prompts_path\u001b[38;5;241m=\u001b[39mprompts_path)\n\u001b[0;32m--> 167\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_multiple_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m meta_evaluator \u001b[38;5;241m=\u001b[39m MetaEvaluator()\n\u001b[1;32m    171\u001b[0m meta_test_cases \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:215\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.evaluate_multiple_samples\u001b[0;34m(self, eval_samples, semaphore_size)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_multiple_samples\u001b[39m(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m, eval_samples: List[EvaluationSample], semaphore_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m    214\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[GroundedQAEvaluation]:\n\u001b[0;32m--> 215\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_evaluate_multiple_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemaphore_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:209\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.async_evaluate_multiple_samples\u001b[0;34m(self, eval_samples, semaphore_size)\u001b[0m\n\u001b[1;32m    202\u001b[0m semaphore \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mSemaphore(semaphore_size)\n\u001b[1;32m    203\u001b[0m evaluation_coroutines \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    204\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__evaluate_sample_with_semaphore(eval_sample, semaphore)\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_sample \u001b[38;5;129;01min\u001b[39;00m eval_samples\n\u001b[1;32m    208\u001b[0m ]\n\u001b[0;32m--> 209\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mevaluation_coroutines)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evaluations\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/tasks.py:571\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:197\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.__evaluate_sample_with_semaphore\u001b[0;34m(self, sample, semaphore)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__evaluate_sample_with_semaphore\u001b[39m(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m, sample: EvaluationSample, semaphore: asyncio\u001b[38;5;241m.\u001b[39mSemaphore\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_single_sample(sample)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:160\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.evaluate_single_sample\u001b[0;34m(self, eval_sample)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_single_sample\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m, eval_sample: EvaluationSample\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GroundedQAEvaluation:\n\u001b[1;32m    159\u001b[0m     answer_relevancy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_answer_relevancy(eval_sample)\n\u001b[0;32m--> 160\u001b[0m     completeness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_completeness(eval_sample)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(answer_relevancy, Failed):\n\u001b[1;32m    163\u001b[0m         usefulness \u001b[38;5;241m=\u001b[39m Failed(error\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_relevancy failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:132\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.evaluate_completeness\u001b[0;34m(self, eval_sample)\u001b[0m\n\u001b[1;32m    125\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mget_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleteness.txt.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39meval_sample\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m    128\u001b[0m     actual_output\u001b[38;5;241m=\u001b[39meval_sample\u001b[38;5;241m.\u001b[39mactual_output,\n\u001b[1;32m    129\u001b[0m     expected_output\u001b[38;5;241m=\u001b[39meval_sample\u001b[38;5;241m.\u001b[39mexpected_output,\n\u001b[1;32m    130\u001b[0m     contexts\u001b[38;5;241m=\u001b[39meval_sample\u001b[38;5;241m.\u001b[39mreferences,\n\u001b[1;32m    131\u001b[0m )\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_llm(prompt, CompletenessPair)\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/grouse/grounded_qa_evaluator.py:81\u001b[0m, in \u001b[0;36mGroundedQAEvaluator.call_llm\u001b[0;34m(self, prompt, pair_model)\u001b[0m\n\u001b[1;32m     79\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m}\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4o\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name:\n\u001b[0;32m---> 81\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39macompletion(\n\u001b[1;32m     82\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     83\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m     84\u001b[0m         response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39macompletion(\n\u001b[1;32m     89\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m     90\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/utils.py:1460\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1458\u001b[0m timeout \u001b[38;5;241m=\u001b[39m _get_wrapper_timeout(kwargs\u001b[38;5;241m=\u001b[39mkwargs, exception\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout)\n\u001b[0;32m-> 1460\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/utils.py:1321\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper_async\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _caching_handler_response\u001b[38;5;241m.\u001b[39mfinal_embedding_cached_response\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1321\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1322\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/main.py:496\u001b[0m, in \u001b[0;36macompletion\u001b[0;34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    495\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/miniforge3/envs/ecc/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:429\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m    428\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m    430\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    431\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    432\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    433\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    434\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[1;32m    437\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-JV6yPaNcUy5jq93eBhjTM7ed on tokens per min (TPM): Limit 200000, Used 199578, Requested 2218. Please try again in 538ms. Visit https://platform.openai.com/account/rate-limits to learn more."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_evaluations = meta_evaluate_pipeline(\"gpt-4o-mini\", prompts_path, train_set=False)\n",
    "meta_evaluations.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests can help you assess the limits of your judge LLM on edge cases but don't guarantee that your judge LLM will be perfect. Be cautious when analysing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```latex\n",
    "@misc{muller2024grousebenchmarkevaluateevaluators,\n",
    "      title={GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering}, \n",
    "      author={Sacha Muller and António Loison and Bilel Omrani and Gautier Viaud},\n",
    "      year={2024},\n",
    "      eprint={2409.06595},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL},\n",
    "      url={https://arxiv.org/abs/2409.06595}, \n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
